{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math,random\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from src.features.build_features import word_count, sentence_avg_word_length, normalize\n",
    "from src.features.text_blob_analysis import analyze_sentiment, analyze_word_class\n",
    "from src.data.make_dataset import create_dataset\n",
    "from src.data.util import unzip_file\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "\n",
    "def perceptron_output(weights, bias, x):\n",
    "    '''Returns 1 if the perceptrion 'fires', 0 if not '''\n",
    "    return step_function(np.dot(weights, x) + bias)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(np.dot(weights, inputs))\n",
    "\n",
    "def predict(input, network):\n",
    "    return feed_forward(network, input)[-1]\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "def backpropagate(network, input_vector, targets):\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    \n",
    "  \n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target) for output, target in zip(outputs, targets)]\n",
    "        # adjust weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "    # focus on the ith output layer neuron\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # adjust the jth weight based on both\n",
    "            # this neuron's delta and its jth input\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) * np.dot(output_deltas, [n[i] for n in output_layer])for i, hidden_output in enumerate(hidden_outputs)]\n",
    "        \n",
    "    # adjust weights for hidden layer, one neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating missing paths...\n",
      "Skipping unzip...\n",
      "Skipping data filtering...\n",
      "750\n",
      "       genre                                             lyrics\n",
      "745     Rock  my strange uncles from abroad yes i never met'...\n",
      "746  Hip-Hop  heyah heyah heyah heyah hey i'm thinking about...\n",
      "747  Hip-Hop  ladies and gentlemen we got de la up in the ho...\n",
      "748  Hip-Hop  this one's for all the leaders leader lets all...\n",
      "749      Pop  blue painted sea watercolors in my dreams a pu...\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "test_df, df = create_dataset()\n",
    "#test_df, df = create_dataset()\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "#print(len(testdata_df))\n",
    "df = df.sample(frac=1).reset_index(drop=True)#\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:01<00:00, 672.26it/s]\n",
      "Analyzing sentiment...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 752746.59it/s]\n",
      "Analyzing sentiment...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 719023.54it/s]\n",
      "Preparing Text class analysis...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:11<00:00, 65.47it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 46997.46it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 53717.12it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 47003.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2079002079002079, 0.07499999999999998, 0.31250000000000006, 0.63, 0.09, 0.09, 0.01888758920124565], [0.4178794178794179, -0.023524305555555555, 0.5927951388888889, 0.73, 0.42, 0.32, 0.004637961089191585], [0.2702702702702703, -0.00788690476190476, 0.5146825396825395, 0.67, 0.35, 0.14, 0.011110226424631933], [0.2182952182952183, -0.1270408163265306, 0.464030612244898, 0.52, 0.11, 0.18, 0.01489337311827869], [0.07588357588357589, -0.09583333333333333, 0.3354166666666667, 0.25, 0.13, 0.11, 0.07613477230145324], [0.19542619542619544, 0.08166666666666668, 0.5711111111111111, 0.3, 0.52, 0.17, 0.019701193180852266], [0.31496881496881496, 0.2128384687208217, 0.6376750700280113, 0.5, 0.09, 0.38, 0.007963665384534248], [0.6018711018711018, 0.006659512803580602, 0.540883104654291, 1.28, 0.48, 0.39, 0.0017418571090835314], [0.632016632016632, 0.05679563492063492, 0.6968253968253967, 1.41, 0.54, 0.56, 0.001552089144493869], [0.17255717255717257, 0.484589947089947, 0.8312334656084657, 0.41, 0.11, 0.21, 0.023299513453291714]]\n"
     ]
    }
   ],
   "source": [
    "# targets\n",
    "series = df['genre'].value_counts()\n",
    "genre_labels = series.keys() # getting genre labels\n",
    "targets = [[1 if i == j else 0 for i in genre_labels] for j in df['genre']]\n",
    "\n",
    "# features\n",
    "df = sentence_avg_word_length(df,\"avg_word_len\", 'lyrics')\n",
    "df = normalize(df, 'avg_word_len_nm', 'avg_word_len')\n",
    "df = word_count(df,\"word_count\", 'lyrics')\n",
    "df = normalize(df, 'word_count_nm', 'word_count')\n",
    "df = analyze_sentiment(df)\n",
    "df = analyze_word_class(df)\n",
    "\n",
    "avg_word_len = df['avg_word_len_nm']\n",
    "words = df[\"word_count_nm\"]\n",
    "polarity = df['polarity']\n",
    "subjectivity = df['subjectivity']\n",
    "nouns = df['nouns']\n",
    "adverbs = df['adverbs']\n",
    "verbs = df['verbs']\n",
    "\n",
    "# Create feature list\n",
    "inputs = [[f, p, s, n, a, v, wl] for f, p, s, n, a, v, wl in zip(words, polarity, subjectivity, nouns, adverbs, verbs, avg_word_len)]\n",
    "print(inputs[0:10])\n",
    "\n",
    "#shuffle(inputs)\n",
    "#inputs = inputs[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.8444218515250481, 0.7579544029403025, 0.420571580830845, 0.25891675029296335, 0.5112747213686085, 0.4049341374504143, 0.7837985890347726, 0.30331272607892745], [0.4765969541523558, 0.5833820394550312, 0.9081128851953352, 0.5046868558173903, 0.28183784439970383, 0.7558042041572239, 0.6183689966753316, 0.25050634136244054], [0.9097462559682401, 0.9827854760376531, 0.8102172359965896, 0.9021659504395827, 0.3101475693193326, 0.7298317482601286, 0.8988382879679935, 0.6839839319154413], [0.47214271545271336, 0.1007012080683658, 0.4341718354537837, 0.6108869734438016, 0.9130110532378982, 0.9666063677707588, 0.47700977655271704, 0.8653099277716401]], [[0.2604923103919594, 0.8050278270130223, 0.5486993038355893, 0.014041700164018955, 0.7197046864039541], [0.39882354222426875, 0.824844977148233, 0.6681532012318508, 0.0011428193144282783, 0.49357786646532464], [0.8676027754927809, 0.24391087688713198, 0.32520436274739006, 0.8704712321086546, 0.19106709150239054]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [01:47<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.819527295055575, 13.47195739914964, -8.188822924429266, 2.282260580460534, -1.9005190928476696, 56.954003819523074, 8.725476067147865, -7.7758977010204005], [34.849244904472386, -9.557263082789964, 15.070479644357885, 17.81977973344491, 9.522707597360547, 11.164769835371091, -3.5180353999144116, -37.07220984472131], [1.8174484737894494, 1.5455668051905904, 3.4917062595367256, 2.9981912227959406, 1.136720369942305, 1.537555416139526, 1.3246029700856543, 6.549482051024189], [1.4867933235235047, 0.5952650581860128, 3.1222006053525964, 2.7370807758618336, 1.9985487350227926, 2.1676136946917177, 0.2624101417984257, 6.8687145432113335]], [[2.4213116099043233, 4.905516833285936, 0.31311947439683835, -0.9535007223950023, -2.2087211236803905], [-2.0685677666289344, -9.56013215333961, -0.41410408463893367, -0.994844437792153, 1.3483688468865092], [0.9130941852792996, -4.032689357566527, -0.5905138616558943, 0.7327804302833754, -0.42990576414057435]]]\n"
     ]
    }
   ],
   "source": [
    "########### Træning af model ###########\n",
    "\n",
    "###########\n",
    "# Opsætning af Neural Network\n",
    "###########\n",
    "random.seed(0) # to get repeatable results\n",
    "input_size = 7 # antal af input noder (samme antal som feautures)\n",
    "num_hidden = 5 # antal af hidden noder\n",
    "output_size = 3 # antal af output noder (i vores tilfælde, genres)\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)] for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)] for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "# Iteration of training\n",
    "#num = 0\n",
    "print(network)\n",
    "for __ in  tqdm(range(3000)):\n",
    "    #num = num +1\n",
    "    #if num == 200 or num == 1000 or num == 1500 or num == 2000 or num == 3500:\n",
    "     #   print(network)\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 642.58it/s]\n",
      "Analyzing sentiment...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 100150.53it/s]\n",
      "Analyzing sentiment...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 100246.27it/s]\n",
      "Preparing Text class analysis...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 56.95it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 25079.55it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 20056.92it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 50171.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "test_df = test_df.copy()\n",
    "test_df = test_df.sample(100)\n",
    "\n",
    "test_df = sentence_avg_word_length(test_df,\"avg_word_len\", 'lyrics')\n",
    "test_df = normalize(test_df, 'avg_word_len_nm', 'avg_word_len')\n",
    "\n",
    "test_df = word_count(test_df,\"word_count\", 'lyrics')\n",
    "test_df = normalize(test_df, 'word_count_nm', 'word_count')\n",
    "test_df = analyze_sentiment(test_df)\n",
    "test_df = analyze_word_class(test_df)\n",
    "\n",
    "avg_word_len = test_df['avg_word_len_nm']\n",
    "words = test_df[\"word_count_nm\"]\n",
    "polarity = test_df['polarity']\n",
    "subjectivity = test_df['subjectivity']\n",
    "nouns = test_df['nouns']\n",
    "adverbs = test_df['adverbs']\n",
    "verbs = test_df['verbs']\n",
    "\n",
    "# Create feature list\n",
    "test_features = [[f, p, s, n, a, v, wl] for f, p, s, n, a, v, wl in zip(words, polarity, subjectivity, nouns, adverbs, verbs, avg_word_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Index(['Hip-Hop', 'Rock', 'Pop'], dtype='object')\n",
      "[0.8984977102605137, 0.000736710633906047, 0.17962886371791117]\n"
     ]
    }
   ],
   "source": [
    "# 'Rock', 'Pop', 'Hip-Hop', 'Not Available', 'Metal', 'Country', 'Jazz', 'Electronic', 'Other', 'R&B', 'Indie', 'Folk'\n",
    "\n",
    "\n",
    "print('##########################')\n",
    "print(genre_labels)\n",
    "res = predict([0.71148825065274152, 0.22561965811965812, 0.129914529914531, 0.2506896551724138, 0.0513455968010067,1,1], network)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9887683688235016, 8.383140592861208e-06, 0.032070136192982455]\n",
      "Hip-Hop\n",
      "[0.057575119204955186, 0.4734168569009032, 0.43354696297176304]\n",
      "Rock\n",
      "[0.9887682853089413, 8.383276229282182e-06, 0.03207033485992916]\n",
      "Hip-Hop\n",
      "[0.7545102810303407, 0.005743451466870997, 0.34244281456066056]\n",
      "Hip-Hop\n",
      "[0.9886684602261593, 8.447537088030916e-06, 0.031965448645637246]\n",
      "Hip-Hop\n",
      "[0.703743109530164, 0.009463532802088077, 0.39186419449494675]\n",
      "Hip-Hop\n",
      "[0.9882326234917497, 8.72803955266045e-06, 0.031524359125590276]\n",
      "Hip-Hop\n",
      "[0.0644486101772123, 0.44797509315179945, 0.44469294046636576]\n",
      "Rock\n",
      "[0.39015063597799915, 0.10784223602879794, 0.6498366341930782]\n",
      "Pop\n",
      "[0.9887683703520171, 8.383143263889552e-06, 0.0320701354758099]\n",
      "Hip-Hop\n",
      "[0.3946498805657231, 0.10630168623088776, 0.6514564222275997]\n",
      "Pop\n",
      "[0.9887683700426303, 8.383143367056846e-06, 0.032070136746813034]\n",
      "Hip-Hop\n",
      "[0.9887497384030194, 8.410582297225196e-06, 0.03211284745121219]\n",
      "Hip-Hop\n",
      "[0.9887679241982827, 8.383829815995546e-06, 0.03207117689085404]\n",
      "Hip-Hop\n",
      "[0.9876630547378575, 1.0084063453454529e-05, 0.03457062391058206]\n",
      "Hip-Hop\n",
      "[0.38655926889660885, 0.10909288004311059, 0.6485379476463832]\n",
      "Pop\n",
      "[0.39465290100023975, 0.10630282110445245, 0.6514554032818859]\n",
      "Pop\n",
      "[0.39228828570179664, 0.10711995930018171, 0.650601715666495]\n",
      "Pop\n",
      "[0.37996639411291133, 0.111429969795026, 0.6461375075324988]\n",
      "Pop\n",
      "[0.9887663956853342, 8.38613013048885e-06, 0.03207473383924218]\n",
      "Hip-Hop\n",
      "[0.08791331643700728, 0.378486747203396, 0.4761573473604212]\n",
      "Pop\n",
      "[0.166979049083836, 0.24572833461728769, 0.5450490716750223]\n",
      "Pop\n",
      "[0.3563513227707961, 0.12031519319986084, 0.637333038593882]\n",
      "Pop\n",
      "[0.05485335376502667, 0.48436920574690767, 0.42880170518526795]\n",
      "Rock\n",
      "[0.3946281720027875, 0.1063100794215509, 0.6514454801856264]\n",
      "Pop\n",
      "[0.08318957986282877, 0.39069393153877957, 0.4704821230153774]\n",
      "Pop\n",
      "[0.37591094337111053, 0.11289980403016979, 0.6446455086774757]\n",
      "Pop\n",
      "[0.9887661817393464, 8.386397392308366e-06, 0.03207518878160811]\n",
      "Hip-Hop\n",
      "[0.9887683729058154, 8.383150309971105e-06, 0.03207013821115968]\n",
      "Hip-Hop\n",
      "[0.39260436890374606, 0.10699967117279109, 0.6507176476991263]\n",
      "Pop\n",
      "[0.05774568235854063, 0.4727434161579865, 0.4338391210681718]\n",
      "Rock\n",
      "[0.9886140433436812, 8.611757162975829e-06, 0.0324242629095427]\n",
      "Hip-Hop\n",
      "[0.9882378360603618, 8.724295416941145e-06, 0.03152805065064714]\n",
      "Hip-Hop\n",
      "[0.056251674169168694, 0.47868917289104496, 0.4312561468349957]\n",
      "Rock\n",
      "[0.9887682461061446, 8.383243326531058e-06, 0.032070011948138036]\n",
      "Hip-Hop\n",
      "[0.09833582751136317, 0.35405262445345725, 0.48777736738572247]\n",
      "Pop\n",
      "[0.9887683706221633, 8.38314520109598e-06, 0.03207013789962371]\n",
      "Hip-Hop\n",
      "[0.3965902241069656, 0.10480840165142448, 0.6499449287411619]\n",
      "Pop\n",
      "[0.988768368960143, 8.383141049068854e-06, 0.032070136509514666]\n",
      "Hip-Hop\n",
      "[0.3958981056513072, 0.10511066322222169, 0.6498850860164425]\n",
      "Pop\n",
      "[0.9887683705280031, 8.383144663247308e-06, 0.03207013730247367]\n",
      "Hip-Hop\n",
      "[0.9887683708759131, 8.383145770890827e-06, 0.0320701369663786]\n",
      "Hip-Hop\n",
      "[0.9887682969177601, 8.383203142314537e-06, 0.03207005299520886]\n",
      "Hip-Hop\n",
      "[0.988768371562159, 8.383146156863538e-06, 0.03207013623814094]\n",
      "Hip-Hop\n",
      "[0.40312810512867864, 0.09992890756929378, 0.644832809874675]\n",
      "Pop\n",
      "[0.9799822035400657, 1.3839717840367645e-05, 0.025868353354125514]\n",
      "Hip-Hop\n",
      "[0.39800242267719094, 0.10373392701385879, 0.648841698476521]\n",
      "Pop\n",
      "[0.10618653380821454, 0.3374777975042108, 0.49585442387888967]\n",
      "Pop\n",
      "[0.39342203298465145, 0.10672278880653267, 0.6510125274592626]\n",
      "Pop\n",
      "[0.9887683695607299, 8.383141608301493e-06, 0.032070135130143174]\n",
      "Hip-Hop\n",
      "[0.9833827968142906, 1.8177401907404654e-05, 0.04390628124210023]\n",
      "Hip-Hop\n",
      "[0.39459622069302935, 0.10627961405190811, 0.6513391290057274]\n",
      "Pop\n",
      "[0.36989579195534594, 0.11511876877516146, 0.6424236102349453]\n",
      "Pop\n",
      "[0.05483371790029927, 0.4844649324657893, 0.42875797237644625]\n",
      "Rock\n",
      "[0.9887683691555805, 8.383141518596234e-06, 0.03207013664163515]\n",
      "Hip-Hop\n",
      "[0.39785432226574197, 0.10383149281418617, 0.6489172057801968]\n",
      "Pop\n",
      "[0.9887683466853616, 8.383163139619468e-06, 0.03207010486061301]\n",
      "Hip-Hop\n",
      "[0.9887683716177049, 8.383144739923728e-06, 0.0320701330804577]\n",
      "Hip-Hop\n",
      "[0.0561056657042376, 0.4792730272428435, 0.43100737563179914]\n",
      "Rock\n",
      "[0.9887678874631726, 8.383454227020406e-06, 0.032069625337376936]\n",
      "Hip-Hop\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test, t in zip(test_features, test_df['genre']):\n",
    "    test_res = predict(test,network)\n",
    "    \n",
    "    maxnum = test_res.index(max(test_res))\n",
    "    if genre_labels[maxnum] == t:\n",
    "        count = count +1\n",
    "        print(test_res)\n",
    "        print(t)\n",
    "        \n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
