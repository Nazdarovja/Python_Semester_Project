{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math,random\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from src.features.build_features import word_count, sentence_avg_word_length, normalize\n",
    "from src.features.text_blob_analysis import analyze_sentiment, analyze_word_class\n",
    "from src.data.make_dataset import create_dataset\n",
    "from src.data.util import unzip_file\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "\n",
    "def perceptron_output(weights, bias, x):\n",
    "    '''Returns 1 if the perceptrion 'fires', 0 if not '''\n",
    "    return step_function(np.dot(weights, x) + bias)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(np.dot(weights, inputs))\n",
    "\n",
    "def predict(input, network):\n",
    "    return feed_forward(network, input)[-1]\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "def backpropagate(network, input_vector, targets):\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    \n",
    "  \n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target) for output, target in zip(outputs, targets)]\n",
    "        # adjust weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "    # focus on the ith output layer neuron\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # adjust the jth weight based on both\n",
    "            # this neuron's delta and its jth input\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) * np.dot(output_deltas, [n[i] for n in output_layer])for i, hidden_output in enumerate(hidden_outputs)]\n",
    "        \n",
    "    # adjust weights for hidden layer, one neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating missing paths...\n",
      "Skipping unzip...\n",
      "Skipping data filtering...\n",
      "750\n",
      "       genre                                             lyrics\n",
      "745  Hip-Hop  peace this is spike lee aka shelton jackson le...\n",
      "746  Hip-Hop  you out there louder well clap your hands to w...\n",
      "747  Hip-Hop  silk smooth how i move but i'm gonna do do wha...\n",
      "748      Pop  swore i count  of the dead of the night a love...\n",
      "749     Rock  got your pride and your prose tucked just like...\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "test_df, df = create_dataset()\n",
    "#test_df, df = create_dataset()\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "#print(len(testdata_df))\n",
    "df = df.sample(frac=1).reset_index(drop=True)#\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:04<00:00, 171.97it/s]\n",
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 125302.85it/s]\n",
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 150426.93it/s]\n",
      "Preparing Text class analysis...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:35<00:00, 21.05it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 9283.95it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 13428.13it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 12423.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13513513513513514, 0.2845588235294118, 0.5355392156862745, 0.34, 0.09, 0.11, 0.03217586697768667], [0.5218295218295218, 0.08303589834202083, 0.46652899254940083, 1.49, 0.31, 0.39, 0.0030785883347231708], [0.5758835758835759, 0.14672430743859316, 0.49861521698256395, 1.18, 0.43, 0.5, 0.002118704857859705], [0.32536382536382535, 0.031563786008230454, 0.4622633744855966, 0.42, 0.34, 0.3, 0.00691362556108521], [0.23076923076923078, 0.19999999999999996, 0.354, 0.61, 0.11, 0.05, 0.015621890672178227], [0.20686070686070687, 0.22566844919786108, 0.5039215686274511, 0.32, 0.74, 0.36, 0.020415656343342473], [0.2515592515592516, 0.1736111111111111, 0.33472222222222225, 0.57, 0.36, 0.04, 0.012722854082660263], [0.2588357588357588, 0.1618055555555556, 0.37986111111111104, 0.72, 0.21, 0.31, 0.013170316844755645], [0.23284823284823286, 0.13604166666666667, 0.5635416666666667, 0.5, 0.31, 0.36, 0.014332772674146092], [0.16735966735966737, 0.07910466269841267, 0.5955977182539683, 0.23, 0.27, 0.25, 0.02314842903439061]]\n"
     ]
    }
   ],
   "source": [
    "# targets\n",
    "series = df['genre'].value_counts()\n",
    "genre_labels = series.keys() # getting genre labels\n",
    "targets = [[1 if i == j else 0 for i in genre_labels] for j in df['genre']]\n",
    "\n",
    "# features\n",
    "df = sentence_avg_word_length(df,\"avg_word_len\", 'lyrics')\n",
    "df = normalize(df, 'avg_word_len_nm', 'avg_word_len')\n",
    "df = word_count(df,\"word_count\", 'lyrics')\n",
    "df = normalize(df, 'word_count_nm', 'word_count')\n",
    "df = analyze_sentiment(df)\n",
    "df = analyze_word_class(df)\n",
    "\n",
    "avg_word_len = df['avg_word_len_nm']\n",
    "words = df[\"word_count_nm\"]\n",
    "polarity = df['polarity']\n",
    "subjectivity = df['subjectivity']\n",
    "nouns = df['nouns']\n",
    "adverbs = df['adverbs']\n",
    "verbs = df['verbs']\n",
    "\n",
    "# Create feature list\n",
    "inputs = [[f, p, s, n, a, v, wl] for f, p, s, n, a, v, wl in zip(words, polarity, subjectivity, nouns, adverbs, verbs, avg_word_len)]\n",
    "print(inputs[0:10])\n",
    "\n",
    "#shuffle(inputs)\n",
    "#inputs = inputs[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.8444218515250481, 0.7579544029403025, 0.420571580830845, 0.25891675029296335, 0.5112747213686085, 0.4049341374504143, 0.7837985890347726, 0.30331272607892745], [0.4765969541523558, 0.5833820394550312, 0.9081128851953352, 0.5046868558173903, 0.28183784439970383, 0.7558042041572239, 0.6183689966753316, 0.25050634136244054], [0.9097462559682401, 0.9827854760376531, 0.8102172359965896, 0.9021659504395827, 0.3101475693193326, 0.7298317482601286, 0.8988382879679935, 0.6839839319154413], [0.47214271545271336, 0.1007012080683658, 0.4341718354537837, 0.6108869734438016, 0.9130110532378982, 0.9666063677707588, 0.47700977655271704, 0.8653099277716401]], [[0.2604923103919594, 0.8050278270130223, 0.5486993038355893, 0.014041700164018955, 0.7197046864039541], [0.39882354222426875, 0.824844977148233, 0.6681532012318508, 0.0011428193144282783, 0.49357786646532464], [0.8676027754927809, 0.24391087688713198, 0.32520436274739006, 0.8704712321086546, 0.19106709150239054]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [04:24<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.8921955236372352, 1.6370103020745497, 3.6393070110217303, 2.9949735415783114, 1.3482331298476364, 1.0059215930087673, 1.1700806297125212, 6.914125488321733], [18.25204404129903, -0.7397557130003489, 8.746428003305947, 6.3017191213388415, 2.004629640052655, 6.959149379069717, -3.4629212576147053, -17.758807253057824], [44.875308925584925, 3.429502114818368, -5.954117415654864, 13.05925553107093, -3.1788143467360634, 20.100944492311054, -1.4873809571092396, -1.940439311291881], [1.285495122598479, 0.7258172302060236, 3.588997249614815, 2.942499214109537, 1.5269409275839434, 1.2509410291557457, 0.7815409634791093, 7.501375265809378]], [[-3.2425932317620396, 5.266694447572339, 11.819605918775345, -4.841237207302153, -6.017787761345706], [-4.638950292456452, -3.169899516221693, 13.924113235810754, -4.576276885433242, -4.222090455480682], [2.8537139904863693, -3.8252488788625953, -12.30179119905892, 3.627960921889604, 4.649104947523585]]]\n"
     ]
    }
   ],
   "source": [
    "########### Træning af model ###########\n",
    "\n",
    "###########\n",
    "# Opsætning af Neural Network\n",
    "###########\n",
    "random.seed(0) # to get repeatable results\n",
    "input_size = 7 # antal af input noder (samme antal som feautures)\n",
    "num_hidden = 4 # antal af hidden noder\n",
    "output_size = 3 # antal af output noder (i vores tilfælde, genres)\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)] for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)] for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "# Iteration of training\n",
    "#num = 0\n",
    "print(network)\n",
    "for __ in  tqdm(range(3000)):\n",
    "    #num = num +1\n",
    "    #if num == 200 or num == 1000 or num == 1500 or num == 2000 or num == 3500:\n",
    "     #   print(network)\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 231.03it/s]\n",
      "Analyzing sentiment...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 97723.77it/s]\n",
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 101606.20it/s]\n",
      "Preparing Text class analysis...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.31it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 7156.78it/s]\n",
      "Analyzing classes...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 9115.87it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 16703.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "test_df = test_df.copy()\n",
    "test_df = test_df.sample(100)\n",
    "\n",
    "test_df = sentence_avg_word_length(test_df,\"avg_word_len\", 'lyrics')\n",
    "test_df = normalize(test_df, 'avg_word_len_nm', 'avg_word_len')\n",
    "\n",
    "test_df = word_count(test_df,\"word_count\", 'lyrics')\n",
    "test_df = normalize(test_df, 'word_count_nm', 'word_count')\n",
    "test_df = analyze_sentiment(test_df)\n",
    "test_df = analyze_word_class(test_df)\n",
    "\n",
    "avg_word_len = test_df['avg_word_len_nm']\n",
    "words = test_df[\"word_count_nm\"]\n",
    "polarity = test_df['polarity']\n",
    "subjectivity = test_df['subjectivity']\n",
    "nouns = test_df['nouns']\n",
    "adverbs = test_df['adverbs']\n",
    "verbs = test_df['verbs']\n",
    "\n",
    "# Create feature list\n",
    "test_features = [[f, p, s, n, a, v, wl] for f, p, s, n, a, v, wl in zip(words, polarity, subjectivity, nouns, adverbs, verbs, avg_word_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Index(['Hip-Hop', 'Pop', 'Rock'], dtype='object')\n",
      "[0.8724030346200379, 0.11469677164895586, 0.01441877759447464]\n"
     ]
    }
   ],
   "source": [
    "# 'Rock', 'Pop', 'Hip-Hop', 'Not Available', 'Metal', 'Country', 'Jazz', 'Electronic', 'Other', 'R&B', 'Indie', 'Folk'\n",
    "\n",
    "\n",
    "print('##########################')\n",
    "print(genre_labels)\n",
    "res = predict([0.71148825065274152, 0.22561965811965812, 0.129914529914531, 0.2506896551724138, 0.0513455968010067,1,1], network)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.679547085332456e-06, 6.529792045482793e-06, 0.9999449485642234]\n",
      "Rock\n",
      "[0.9517397089627953, 0.0640862255092327, 0.006732310938257045]\n",
      "Hip-Hop\n",
      "[0.004409017027710538, 0.038805523669949146, 0.8903921015258296]\n",
      "Rock\n",
      "[0.11669870011492328, 0.5821526221912914, 0.20450706951153577]\n",
      "Pop\n",
      "[0.9518384619632565, 0.06400867514017809, 0.006721866026586713]\n",
      "Hip-Hop\n",
      "[0.09309094123328249, 0.6176367686113597, 0.23607213121452403]\n",
      "Pop\n",
      "[0.09326266786720518, 0.6183022638513951, 0.2356785232524625]\n",
      "Pop\n",
      "[0.10386063458453655, 0.6012086424837763, 0.22041759543066838]\n",
      "Pop\n",
      "[0.08360565399959452, 0.587636602794291, 0.2584555908596763]\n",
      "Pop\n",
      "[0.7473252471538561, 0.1766330123466279, 0.026184154459558365]\n",
      "Hip-Hop\n",
      "[0.95187699589741, 0.06397805831580307, 0.006717791061717151]\n",
      "Hip-Hop\n",
      "[0.09738600764309358, 0.6114915799671357, 0.22947909621681592]\n",
      "Pop\n",
      "[0.9063096874848148, 0.09512860987831084, 0.011242640694851683]\n",
      "Hip-Hop\n",
      "[0.35049168112452905, 0.3739325142456354, 0.08467842978499289]\n",
      "Pop\n",
      "[0.9518746306341697, 0.06398005715510005, 0.0067180405385368095]\n",
      "Hip-Hop\n",
      "[0.0011526500030549002, 0.008199546096633835, 0.9705360532331633]\n",
      "Rock\n",
      "[0.9506634112468465, 0.06492894182364914, 0.006845870838229796]\n",
      "Hip-Hop\n",
      "[0.9518764162058292, 0.06397858666849321, 0.006717851957961892]\n",
      "Hip-Hop\n",
      "[0.08779040386673383, 0.6015897345142833, 0.248105181890411]\n",
      "Pop\n",
      "[0.1909228459828084, 0.49562579325962947, 0.14434599995789332]\n",
      "Pop\n",
      "[0.0837188332121054, 0.5876196433633326, 0.2582167242966753]\n",
      "Pop\n",
      "[0.951877304478789, 0.06397775303020034, 0.006717758742160032]\n",
      "Hip-Hop\n",
      "[0.9148104651039616, 0.08985372121802389, 0.010429835970255161]\n",
      "Hip-Hop\n",
      "[0.9518725053274502, 0.06398165051975943, 0.006718265859949393]\n",
      "Hip-Hop\n",
      "[0.9518772133741641, 0.06397798774421258, 0.0067177675195180765]\n",
      "Hip-Hop\n",
      "[0.9518772742403114, 0.06397783895028968, 0.006717761598133767]\n",
      "Hip-Hop\n",
      "[0.0002109380676659809, 0.0011174314118782116, 0.9948465676650949]\n",
      "Rock\n",
      "[0.09752122814245667, 0.6112523036985233, 0.2292823972141947]\n",
      "Pop\n",
      "[0.9486676860615677, 0.06647164687407656, 0.007055183401385011]\n",
      "Hip-Hop\n",
      "[0.10051492273954542, 0.6064510654330817, 0.22500330918658681]\n",
      "Pop\n",
      "[0.9518461823934071, 0.06400266385497998, 0.0067210489808753205]\n",
      "Hip-Hop\n",
      "[0.8697436331713964, 0.1161469185810833, 0.01466527631777448]\n",
      "Hip-Hop\n",
      "[0.9518703191149946, 0.06398343036483388, 0.0067184968718257045]\n",
      "Hip-Hop\n",
      "[0.9518770602869615, 0.06397816273590563, 0.006717783359475632]\n",
      "Hip-Hop\n",
      "[2.0002548444964095e-05, 6.97053927320856e-05, 0.9995540880303299]\n",
      "Rock\n",
      "[0.8936096239632743, 0.1027031479730061, 0.01244199790634707]\n",
      "Hip-Hop\n",
      "[0.0001953138504377303, 0.001020578718923296, 0.9952415434660427]\n",
      "Rock\n",
      "[0.00027702686137012306, 0.0015395976417756523, 0.9931676080204451]\n",
      "Rock\n",
      "[0.9515373969407281, 0.06424547667095602, 0.006753692728823465]\n",
      "Hip-Hop\n",
      "[1.033001657468393e-06, 2.1242305712755186e-06, 0.9999795858176246]\n",
      "Rock\n",
      "[0.18089929029860236, 0.5055882433102952, 0.15039069867636756]\n",
      "Pop\n",
      "[0.9518773112996148, 0.0639777739826514, 0.00671775788124643]\n",
      "Hip-Hop\n",
      "[0.9517368981582707, 0.0640884336613091, 0.006732608153851434]\n",
      "Hip-Hop\n",
      "[0.8097401655572689, 0.146993961073688, 0.020225274008737828]\n",
      "Hip-Hop\n",
      "[0.0952830353594542, 0.6145317233691195, 0.23264891242528876]\n",
      "Pop\n",
      "[3.846877562175051e-06, 9.99782179763982e-06, 0.999919791015743]\n",
      "Rock\n",
      "[0.951863744751325, 0.06398859699853247, 0.006719192424585833]\n",
      "Hip-Hop\n",
      "[0.008953794868954184, 0.08551729741781589, 0.794560230181411]\n",
      "Rock\n",
      "[0.9518773069076623, 0.06397776014093638, 0.006717758437800626]\n",
      "Hip-Hop\n",
      "[0.07093123241206511, 0.5360820077781212, 0.2955358433633576]\n",
      "Pop\n",
      "[0.0017492395930526804, 0.013363518915255716, 0.9551996977879849]\n",
      "Rock\n",
      "[0.1176136200701378, 0.5808865085542166, 0.20346266721193895]\n",
      "Pop\n",
      "[0.08198577299646806, 0.5815710826143602, 0.2627251759880932]\n",
      "Pop\n",
      "[0.9500089216741272, 0.06543775859221185, 0.006914689578023079]\n",
      "Hip-Hop\n",
      "[0.8962051124815354, 0.1011812820605891, 0.012198099633505776]\n",
      "Hip-Hop\n",
      "[0.21666391611215619, 0.4717574164821167, 0.13067565913004142]\n",
      "Pop\n",
      "[0.9445663188270885, 0.06956985778865535, 0.007480575102790373]\n",
      "Hip-Hop\n",
      "[0.015589800943451856, 0.1533267465649738, 0.6832000489656833]\n",
      "Rock\n",
      "[0.08829569280151853, 0.6013938558600005, 0.24716047333979432]\n",
      "Pop\n",
      "[0.012460886158858339, 0.12173818346614877, 0.7320289080952732]\n",
      "Rock\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test, t in zip(test_features, test_df['genre']):\n",
    "    test_res = predict(test,network)\n",
    "    \n",
    "    maxnum = test_res.index(max(test_res))\n",
    "    if genre_labels[maxnum] == t:\n",
    "        count = count +1\n",
    "        print(test_res)\n",
    "        print(t)\n",
    "        \n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
