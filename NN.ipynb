{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math,random\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from src.features.build_features import word_count, sentence_avg_word_length, normalize\n",
    "from src.features.text_blob_analysis import analyze_sentiment, analyze_word_class\n",
    "from src.data.make_dataset import create_dataset\n",
    "from src.data.util import unzip_file\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "\n",
    "def perceptron_output(weights, bias, x):\n",
    "    '''Returns 1 if the perceptrion 'fires', 0 if not '''\n",
    "    return step_function(np.dot(weights, x) + bias)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(np.dot(weights, inputs))\n",
    "\n",
    "def predict(input, network):\n",
    "    return feed_forward(network, input)[-1]\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "def backpropagate(network, input_vector, targets):\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    \n",
    "  \n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target) for output, target in zip(outputs, targets)]\n",
    "        # adjust weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "    # focus on the ith output layer neuron\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # adjust the jth weight based on both\n",
    "            # this neuron's delta and its jth input\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) * np.dot(output_deltas, [n[i] for n in output_layer])for i, hidden_output in enumerate(hidden_outputs)]\n",
    "        \n",
    "    # adjust weights for hidden layer, one neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating missing paths...\n",
      "Skipping unzip...\n",
      "Skipping data filtering...\n",
      "750\n",
      "       genre                                             lyrics\n",
      "745  Hip-Hop  chorus) (flo-rida) this for my ballas ayyyy(4)...\n",
      "746      Pop  you're lying - baby why are you breaking my he...\n",
      "747  Hip-Hop  10 bad bitches in a mansion wrist on milly roc...\n",
      "748     Rock  if i was a hero a superhero i'd know what to d...\n",
      "749     Rock  say you're gonna take me up and down your trac...\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "test_df, df = create_dataset()\n",
    "#test_df, df = create_dataset()\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "#print(len(testdata_df))\n",
    "df = df.sample(frac=1).reset_index(drop=True)#\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:01<00:00, 700.95it/s]\n",
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 744374.82it/s]\n",
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 750770.41it/s]\n",
      "Preparing Text class analysis...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:13<00:00, 56.63it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 53589.91it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 57751.57it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 46891.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3381201044386423, -0.03369483525733524, 0.5218915343915345, 0.19927536231884058, 0.06521739130434782, 0.09420289855072464, 0.1278980245834284], [0.7872062663185379, 0.22907657657657654, 0.38162162162162144, 0.25696594427244585, 0.06501547987616099, 0.05108359133126935, 0.02155606468909279], [0.2950391644908616, 0.010858585858585857, 0.25555555555555554, 0.167420814479638, 0.06334841628959276, 0.04524886877828054, 0.13530119302755017], [0.3289817232375979, 0.20859022556390977, 0.5764411027568922, 0.1625441696113074, 0.06713780918727916, 0.0706713780918728, 0.15239559231217706], [0.195822454308094, 0.10493827160493825, 0.7098765432098767, 0.2289156626506024, 0.060240963855421686, 0.12048192771084337, 0.3567591352145797], [0.2193211488250653, 0.2857142857142857, 0.375, 0.17391304347826086, 0.043478260869565216, 0.010869565217391304, 0.29144219937990945], [0.6005221932114883, 0.4466666666666667, 0.7569047619047617, 0.1392931392931393, 0.031185031185031187, 0.09355509355509356, 0.03767753904491854], [0.4464751958224543, 0.133, 0.26642857142857146, 0.20833333333333334, 0.10555555555555556, 0.06388888888888888, 0.0746119039570181], [0.8433420365535248, -0.062059889109069476, 0.44630146720310626, 0.24962852897473997, 0.06835066864784546, 0.07726597325408618, 0.018964863562933417], [0.26762402088772846, 0.35, 0.6499999999999999, 0.09523809523809523, 0.08095238095238096, 0.16666666666666666, 0.17891359718730188]]\n"
     ]
    }
   ],
   "source": [
    "# targets\n",
    "series = df['genre'].value_counts()\n",
    "genre_labels = series.keys() # getting genre labels\n",
    "targets = [[1 if i == j else 0 for i in genre_labels] for j in df['genre']]\n",
    "\n",
    "# features\n",
    "df = sentence_avg_word_length(df,\"avg_word_len\", 'lyrics')\n",
    "df = normalize(df, 'avg_word_len_nm', 'avg_word_len')\n",
    "df = word_count(df,\"word_count\", 'lyrics')\n",
    "df = normalize(df, 'word_count_nm', 'word_count')\n",
    "df = analyze_sentiment(df)\n",
    "df = analyze_word_class(df)\n",
    "\n",
    "avg_word_len = df['avg_word_len_nm']\n",
    "words = df[\"word_count_nm\"]\n",
    "polarity = df['polarity']\n",
    "subjectivity = df['subjectivity']\n",
    "nouns = df['nouns']\n",
    "adverbs = df['adverbs']\n",
    "verbs = df['verbs']\n",
    "\n",
    "# Create feature list\n",
    "inputs = [[f, p, s, n, a, v, wl] for f, p, s, n, a, v, wl in zip(words, polarity, subjectivity, nouns, adverbs, verbs, avg_word_len)]\n",
    "print(inputs[0:10])\n",
    "\n",
    "#shuffle(inputs)\n",
    "#inputs = inputs[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.8444218515250481, 0.7579544029403025, 0.420571580830845, 0.25891675029296335, 0.5112747213686085, 0.4049341374504143, 0.7837985890347726, 0.30331272607892745], [0.4765969541523558, 0.5833820394550312, 0.9081128851953352, 0.5046868558173903, 0.28183784439970383, 0.7558042041572239, 0.6183689966753316, 0.25050634136244054], [0.9097462559682401, 0.9827854760376531, 0.8102172359965896, 0.9021659504395827, 0.3101475693193326, 0.7298317482601286, 0.8988382879679935, 0.6839839319154413], [0.47214271545271336, 0.1007012080683658, 0.4341718354537837, 0.6108869734438016, 0.9130110532378982, 0.9666063677707588, 0.47700977655271704, 0.8653099277716401]], [[0.2604923103919594, 0.8050278270130223, 0.5486993038355893, 0.014041700164018955, 0.7197046864039541], [0.39882354222426875, 0.824844977148233, 0.6681532012318508, 0.0011428193144282783, 0.49357786646532464], [0.8676027754927809, 0.24391087688713198, 0.32520436274739006, 0.8704712321086546, 0.19106709150239054]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [01:44<00:00, 28.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4.321692884082499, 0.48088974720196015, 3.0865386449069896, 2.113619561667293, 0.5895282193356174, 0.678931824416741, 1.7760446769311464, 6.854479160172247], [-15.779719765068341, 7.414034633773398, -6.622941556253754, -52.83118998589471, -2.0988931517753886, -28.165085422406587, 45.35116127839483, 19.17324715306261], [2.3625059102195447, 1.5915594081474354, 3.232227564663867, 1.863603794092083, 0.4860829727554986, 0.7518658596692722, 2.7395887283726967, 5.551944562019436], [3.8094687669109675, 0.14852104267588645, 2.685584695808479, 2.249465654425198, 1.0569617355745249, 1.3964431068863523, 0.3474753387741677, 7.376136560529309]], [[-1.6237818854742148, 3.382856447746776, -1.9633300220178764, -0.9603849697060493, -0.24522634274240404], [-0.9937410258385413, 4.223823544817371, 1.6475932404755835, -2.2040382398958704, -1.3485029134262285], [2.224679420500646, -5.678605195127497, -0.48243489643763676, 1.6228495314549711, -0.8786300046126648]]]\n"
     ]
    }
   ],
   "source": [
    "########### Træning af model ###########\n",
    "\n",
    "###########\n",
    "# Opsætning af Neural Network\n",
    "###########\n",
    "random.seed(0) # to get repeatable results\n",
    "input_size = 7 # antal af input noder (samme antal som feautures)\n",
    "num_hidden = 4 # antal af hidden noder\n",
    "output_size = 3 # antal af output noder (i vores tilfælde, genres)\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)] for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)] for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "# Iteration of training\n",
    "#num = 0\n",
    "print(network)\n",
    "for __ in  tqdm(range(3000)):\n",
    "    #num = num +1\n",
    "    #if num == 200 or num == 1000 or num == 1500 or num == 2000 or num == 3500:\n",
    "     #   print(network)\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 588.84it/s]\n",
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n",
      "Analyzing sentiment...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n",
      "Preparing Text class analysis...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.27it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 25021.20it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 50009.59it/s]\n",
      "Analyzing classes...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 25028.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "test_df = test_df.copy()\n",
    "test_df = test_df.sample(100)\n",
    "\n",
    "test_df = sentence_avg_word_length(test_df,\"avg_word_len\", 'lyrics')\n",
    "test_df = normalize(test_df, 'avg_word_len_nm', 'avg_word_len')\n",
    "\n",
    "test_df = word_count(test_df,\"word_count\", 'lyrics')\n",
    "test_df = normalize(test_df, 'word_count_nm', 'word_count')\n",
    "test_df = analyze_sentiment(test_df)\n",
    "test_df = analyze_word_class(test_df)\n",
    "\n",
    "avg_word_len = test_df['avg_word_len_nm']\n",
    "words = test_df[\"word_count_nm\"]\n",
    "polarity = test_df['polarity']\n",
    "subjectivity = test_df['subjectivity']\n",
    "nouns = test_df['nouns']\n",
    "adverbs = test_df['adverbs']\n",
    "verbs = test_df['verbs']\n",
    "\n",
    "# Create feature list\n",
    "test_features = [[f, p, s, n, a, v, wl] for f, p, s, n, a, v, wl in zip(words, polarity, subjectivity, nouns, adverbs, verbs, avg_word_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Index(['Pop', 'Hip-Hop', 'Rock'], dtype='object')\n",
      "[0.025855813416636387, 0.9084535626079527, 0.024967467864379127]\n"
     ]
    }
   ],
   "source": [
    "# 'Rock', 'Pop', 'Hip-Hop', 'Not Available', 'Metal', 'Country', 'Jazz', 'Electronic', 'Other', 'R&B', 'Indie', 'Folk'\n",
    "\n",
    "\n",
    "print('##########################')\n",
    "print(genre_labels)\n",
    "res = predict([0.71148825065274152, 0.22561965811965812, 0.129914529914531, 0.2506896551724138, 0.0513455968010067], network)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.012961425073751892, 0.9764636333711986, 0.04056135580320553]\n",
      "Hip-Hop\n",
      "[0.012962007373608959, 0.9764615647060653, 0.04056391448290603]\n",
      "Hip-Hop\n",
      "[0.1951133053613148, 0.1039346880564449, 0.7405371322215246]\n",
      "Rock\n",
      "[0.01639833843067727, 0.9625394670249662, 0.05615749737002833]\n",
      "Hip-Hop\n",
      "[0.013109331395704391, 0.9759277090775832, 0.04120616330953853]\n",
      "Hip-Hop\n",
      "[0.01296227205994509, 0.9764604180189842, 0.040565196462025055]\n",
      "Hip-Hop\n",
      "[0.19555590038430215, 0.10388896475355759, 0.739924290562173]\n",
      "Rock\n",
      "[0.3603777394627713, 0.09366796341187664, 0.5150659618474498]\n",
      "Rock\n",
      "[0.19412729033702533, 0.10402671479795148, 0.741904474798971]\n",
      "Rock\n",
      "[0.19413474504791695, 0.10402507194060963, 0.7418934725421044]\n",
      "Rock\n",
      "[0.013071535637744313, 0.9760799009489268, 0.04100745818542954]\n",
      "Hip-Hop\n",
      "[0.01308504687039503, 0.9760131248864603, 0.04110758968814852]\n",
      "Hip-Hop\n",
      "[0.19412814559509117, 0.10402379738577529, 0.7419029150522146]\n",
      "Rock\n",
      "[0.01297050264553924, 0.9764306033842381, 0.04060146779778115]\n",
      "Hip-Hop\n",
      "[0.19413768941151768, 0.10402354557802711, 0.7418894420729081]\n",
      "Rock\n",
      "[0.19429677349568483, 0.10400751818259418, 0.7416675700906323]\n",
      "Rock\n",
      "[0.012962210585554865, 0.976460674507222, 0.04056489934201416]\n",
      "Hip-Hop\n",
      "[0.1948729384764288, 0.10395463189046165, 0.7408698354508321]\n",
      "Rock\n",
      "[0.07924478017706899, 0.4816700972238253, 0.392429611800669]\n",
      "Hip-Hop\n",
      "[0.19414339987135668, 0.1040247459027127, 0.7418804687972727]\n",
      "Rock\n",
      "[0.5353258618804487, 0.08232244642307177, 0.3280582209086678]\n",
      "Pop\n",
      "[0.20599728575226858, 0.10292840305071084, 0.7253742855321681]\n",
      "Rock\n",
      "[0.19412787710349394, 0.1040260530418049, 0.741902780975671]\n",
      "Rock\n",
      "[0.01449278143875249, 0.9705910802653407, 0.04745660250531618]\n",
      "Hip-Hop\n",
      "[0.013014068955452247, 0.9762722039961965, 0.04079359038904104]\n",
      "Hip-Hop\n",
      "[0.013345210061000739, 0.975050685687691, 0.042263328005090015]\n",
      "Hip-Hop\n",
      "[0.19426434273012505, 0.10401146145843444, 0.7417140021726071]\n",
      "Rock\n",
      "[0.19415442364455973, 0.104022870339736, 0.7418662834413035]\n",
      "Rock\n",
      "[0.19412780697031837, 0.10402517911355141, 0.7419034370651678]\n",
      "Rock\n",
      "[0.19539534823187973, 0.10390560468348697, 0.7401463460307517]\n",
      "Rock\n",
      "[0.517096355305694, 0.0831214576736933, 0.3470268263156891]\n",
      "Pop\n",
      "[0.19412799305571973, 0.10402385617394135, 0.7419034102905652]\n",
      "Rock\n",
      "[0.19727537167951403, 0.10372013135309864, 0.7375439492789291]\n",
      "Rock\n",
      "[0.1959685289570352, 0.10384530414737013, 0.7393516231430953]\n",
      "Rock\n",
      "[0.690725205992027, 0.07428244695365935, 0.18788185207371652]\n",
      "Pop\n",
      "[0.012964653438894067, 0.9764518716048024, 0.04057565231945279]\n",
      "Hip-Hop\n",
      "[0.012971729945145047, 0.9764261343183993, 0.04060690280103337]\n",
      "Hip-Hop\n",
      "[0.03617587401521234, 0.854394419936363, 0.13334173985916387]\n",
      "Hip-Hop\n",
      "[0.19412820011284107, 0.1040243929980467, 0.7419025933607597]\n",
      "Rock\n",
      "[0.5612825091536428, 0.08567382683212531, 0.28483088675393897]\n",
      "Pop\n",
      "[0.013015472358473069, 0.9762669927570784, 0.040800061981505875]\n",
      "Hip-Hop\n",
      "[0.19412832040442013, 0.10402325240869995, 0.7419025721259586]\n",
      "Rock\n",
      "[0.1952728679048134, 0.10391988305831261, 0.7403052553817572]\n",
      "Rock\n",
      "[0.19424138380329714, 0.104014522578227, 0.7417456739258221]\n",
      "Rock\n",
      "[0.013425529444311763, 0.974753888319287, 0.04261360811949151]\n",
      "Hip-Hop\n",
      "[0.1942119292876312, 0.10401661661302024, 0.7417866071938541]\n",
      "Rock\n",
      "[0.06556133318067396, 0.6129563713742661, 0.28887832816449915]\n",
      "Hip-Hop\n",
      "[0.013353626569047358, 0.9750200281189135, 0.04229995189537365]\n",
      "Hip-Hop\n",
      "[0.014675254554467339, 0.9698537090350184, 0.04828828893968936]\n",
      "Hip-Hop\n",
      "[0.19418295681590586, 0.10402166655355338, 0.7418262573845518]\n",
      "Rock\n",
      "[0.1957815722403194, 0.10388217750263384, 0.7395654293944984]\n",
      "Rock\n",
      "[0.27701309339685454, 0.09706434665349668, 0.6296659207492793]\n",
      "Rock\n",
      "[0.04374681784989276, 0.7768513930393426, 0.19550558278846727]\n",
      "Hip-Hop\n",
      "[0.013032825764499397, 0.9762037674461344, 0.04087670922262801]\n",
      "Hip-Hop\n",
      "[0.19455213767966223, 0.10398249445866092, 0.7413155539875125]\n",
      "Rock\n",
      "[0.01300421050528758, 0.9763080418934054, 0.040750274670218155]\n",
      "Hip-Hop\n",
      "[0.012962719501484343, 0.9764590316687244, 0.04056701564287265]\n",
      "Hip-Hop\n",
      "[0.20322205259387824, 0.10315142813280838, 0.7293242885373685]\n",
      "Rock\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test, t in zip(test_features, test_df['genre']):\n",
    "    test_res = predict(test,network)\n",
    "    \n",
    "    maxnum = test_res.index(max(test_res))\n",
    "    if genre_labels[maxnum] == t:\n",
    "        count = count +1\n",
    "        print(test_res)\n",
    "        print(t)\n",
    "        \n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
