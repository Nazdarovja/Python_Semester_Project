{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proof of concept \n",
    "\n",
    "## Predict sang genre, givet sangtekst vha. Naive Bayes\n",
    "\n",
    "### 1. generel opsætning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import random\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def _detect_lan(lyrics):\n",
    "    try:\n",
    "        return detect(lyrics)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "    \n",
    "def _create_word_features(words):\n",
    "    return dict( [ (word, True) for word in words] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. opret DataFrame og rens datasæt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasættet i sin \"raw\" form indeholder:\n",
      "362237 antal rækker\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# import raw dataset\n",
    "csv_file = './data/lyrics.csv'\n",
    "\n",
    "if not os.path.isfile(csv_file):\n",
    "    with zipfile.ZipFile(f'{csv_file}.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./data')\n",
    "        \n",
    "df = pd.read_csv(csv_file)\n",
    "rows, _= df.shape\n",
    "print('datasættet i sin \"raw\" form indeholder:')\n",
    "print(f'{rows} antal rækker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### udvind features og tilføj til datasæt\n",
    "Hvis det er første gang dette step køres vær opmærksom på, at der laves en tung sproglig analyse af sangteksterne. Det vil derfor resulterer i, siden der er MANGE rækker data, at det kan tage 20+ minutter at oversætte alle sangtekster, hvis hele datasættet benyttes. Det kan derfor anbefales, at man tager en mindre sample af datasættet.\n",
    "\n",
    "Hvis dette step ER kørt, så burde der være gemt en .pkl i ./data som gør, at feature genereringen kan springes over.\n",
    "\n",
    "Der skal ligges mærke til, at når der udtages sample af datasættet, så ligges de enkelte udvalgte rækker tilbage. Dette er en IKKE en rigtig måde at gøre det på, da der så kan fremkomme duplikationer af rækker. Det er derimod for, at lave et \"proof of concept\" med mulighed for, at benytte alle genre i træning af modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect language...DONE\n",
      "generate unqiue-words feature...DONE\n",
      "saving feature dataset to pickle...DONE\n"
     ]
    }
   ],
   "source": [
    "FEATURE_DATASET_FILE = './data/feature_dataset.pkl'\n",
    "SAMPLE_SIZE = 1000 # <-- ændre denne hvis et anden størrelse datasæt ønskes\n",
    "\n",
    "if not os.path.isfile(FEATURE_DATASET_FILE):\n",
    "    df_cp = df.copy() # <-- kopi af importeret datasæt\n",
    "\n",
    "    ###\n",
    "    # Oprydning i raw datasæt\n",
    "    ###\n",
    "    # fjern rækker med nan værdier\n",
    "    df_cp = df_cp.dropna()\n",
    "\n",
    "    # fjern uønskede genre (Not Available & Other)\n",
    "    df_cp = df_cp[(df_cp.genre != 'Not Available') & (df_cp.genre != 'Other')]\n",
    "\n",
    "    ###\n",
    "    # Udtaget stikprøve af datasæt\n",
    "    ###\n",
    "    grouped = df_cp.groupby('genre')\n",
    "    grouped = grouped.apply(lambda x: x.sample(n=SAMPLE_SIZE, replace=True))\n",
    "    df_cp = grouped.reset_index(drop=True)\n",
    "    \n",
    "    ###\n",
    "    # Tilføj features til datasæt\n",
    "    ###\n",
    "    # bestem sprog for sangtekst\n",
    "    print('detect language...', end='')\n",
    "    df_cp['lan'] = df_cp.lyrics.apply(_detect_lan)\n",
    "    df_cp = df_cp[df_cp.lan == 'en'] # kun benyt engelske sangtekster\n",
    "    print('DONE')\n",
    "    \n",
    "    # find unikke ord i sangtekst\n",
    "    print('generate unqiue-words feature...', end='')\n",
    "    df_cp['words'] = df_cp.apply(lambda p: (_create_word_features(word_tokenize(p.lyrics)),p.genre), axis=1)\n",
    "    print('DONE')\n",
    "    \n",
    "    # fjern col 'index'\n",
    "    df_cp.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    print('saving feature dataset to pickle...', end='')\n",
    "    df_cp.to_pickle(FEATURE_DATASET_FILE)\n",
    "    print('DONE')\n",
    "else:\n",
    "    print('reading feature dataset from pickle...', end='')\n",
    "    df_cp = pd.read_pickle(FEATURE_DATASET_FILE)\n",
    "    print('DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize to pickle\n",
    "df_cp.to_pickle(\"./data/clean_with_lan.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. klargør ord-feature til Naive Bayes\n",
    "Denne del kan tage +10 min. Sørg derfor IKKE, at overskrive `clean_with_lan_and_words.pkl` med andet! \n",
    "\n",
    "Hvis du har adgang til `clean_with_lan_and_words.pkl` spring dette step over. Det kan ikke svare sig, at oprette datasættet igen. \n",
    "\n",
    "Naive Bayes tager imod en tuple med følgende syntaks `(word_list, stop_word)`, derfor giver det mening af oprette en feature til datasættet som indeholder netop dette.\n",
    "\n",
    "`word_list` er en liste af `tokenized` ord (se `word_tokenize`) og `stop_word` er en string bestående af den genre som ordene tilhører."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indlæs ren, sorteret og med sprog datasettet\n",
    "df = pd.read_pickle('./data/clean_with_lan.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETTE STEP KAN TAGE NOGET TID\n",
    "\n",
    "# DETTE STEP KAN TAGE NOGET TID\n",
    "# serialize to pickle\n",
    "df.to_pickle('./data/clean_with_lan_and_words.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. klargør test-og træningssæt til Naive Bayes og træn\n",
    "Denne del kan kun køres hvis `clean_with_lan_and_words.pkl` datasættet er tilgængeligt. Vær også opmærksom på, at træningen af modellen kan tage noget tid.\n",
    "\n",
    "Der er fri leg for, at træne en model, for alle eller nogle genre. Bestemt størrelse af `training_set` og `test_test`.\n",
    "\n",
    "Når en model er træning, find mulighed for at gemme den! Og find passende navn. Evt. et lille dokument med beskrivelse af hvilke sprog den er trænet med, hvor mange tekster den er trænet med og hvor nøjagtig den er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udpak zippet datasæt hvis den ikke eksisterer\n",
    "if not os.path.isfile('./data/clean_with_lan_and_words.pkl'):\n",
    "    with zipfile.ZipFile('./data/clean_with_lan_and_words.pkl.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./data/')\n",
    "\n",
    "# indlæs ren, sorteret og med sprog og ordliste datasættet\n",
    "df = pd.read_pickle('./data/clean_with_lan_and_words.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opret træningssæt (Kun Pop og Rock)\n",
    "training_set = df[df['genre'] == 'Pop'].words[:1000].tolist() + df[df['genre'] == 'Rock'].words[:1000].tolist()\n",
    "test_set = df[df['genre'] == 'Pop'].words[1000:1250].tolist() + df[df['genre'] == 'Rock'].words[1000:1250].tolist()\n",
    "\n",
    "# bland træningssæt\n",
    "random.shuffle(training_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: , 75.0%\n"
     ]
    }
   ],
   "source": [
    "# DETTE STEP KAN TAGE NOGET TID\n",
    "# Træn Naive Bayes genre-classifieren\n",
    "classifier = NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Bestem nøjagtigheden, ved brug af test_data\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "print(f'Accuracy is: , {accuracy * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop\n"
     ]
    }
   ],
   "source": [
    "# classify en sangtekst\n",
    "word_list, _ = df[df['genre'] == 'Rock'].reset_index().words[1041]\n",
    "result = classifier.classify(word_list)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
