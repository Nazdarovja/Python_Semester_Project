{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_pickle('./data/clean_with_lan_and_words.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, new_col_name, col_to_norm):\n",
    "    '''\n",
    "    ref: https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    max = df[col_to_norm].max()\n",
    "    min = df[col_to_norm].min()\n",
    "\n",
    "    df[new_col_name] = df[col_to_norm].apply(lambda val: (val-min)/(max-min))\n",
    "    return df\n",
    "\n",
    "def _count_words(words):\n",
    "    try:\n",
    "        return len(words.split())\n",
    "    except:\n",
    "        return 0 #TODO: better error handling, maybe not return 0\n",
    "\n",
    "def word_count(df, new_col_name, col_with_lyrics):\n",
    "    df = df.copy()\n",
    "    df[new_col_name] = df[col_with_lyrics].apply(lambda words: _count_words(words))\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df, col_to_process, low=.05, high=.95):\n",
    "    df = df.copy()\n",
    "    min, max = df[col_to_process].quantile([low,high])\n",
    "    df = df[(df[col_to_process] >= min) & (df[col_to_process] <= max)]\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def analyze_sentiment(df):\n",
    "    df = df.copy()\n",
    "    res = df['lyrics'].apply(lambda txt : TextBlob(txt).sentiment)\n",
    "    df['polarity'] = res.apply(lambda x: x[0])\n",
    "    df['subjectivity'] = res.apply(lambda x: x[1])\n",
    "    return df\n",
    "\n",
    "def analyze_word_class(df):\n",
    "    tqdm.pandas(desc=\"Preparing Text class analysis...\")\n",
    "    blobs = df['lyrics'].progress_apply(lambda txt : TextBlob(txt).tags)\n",
    "\n",
    "    tqdm.pandas(desc=\"Analyzing classes...\")\n",
    "    df['nouns'] = blobs.progress_apply(lambda word_list: _count_word_class(word_list, 'NN'))\n",
    "    df['adverbs'] = blobs.progress_apply(lambda word_list: _count_word_class(word_list, 'RB'))\n",
    "    df['verbs'] = blobs.progress_apply(lambda word_list: _count_word_class(word_list, 'VB'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _count_word_class(words, word_class):\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w[1] == word_class:\n",
    "            count = count + 1\n",
    "    return count / 100\n",
    "\n",
    "def prepare_data(df, data_cols, label_col, training_size=1000, test_size=250):\n",
    "    labels = df_cp[label_col].value_counts().keys().tolist()\n",
    "    train_data, train_labels, test_data, test_labels = [], [], [], []\n",
    "    \n",
    "    # shuffle dataset\n",
    "    df = df.copy().sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    for label in labels:\n",
    "        data = df[df[label_col] == label]\n",
    "        # kun hvis der er nok eksempler, ift. training_size og test_size, ud fra den pågældende label\n",
    "        if len(data) > training_size + test_size:\n",
    "            data = data.reset_index(drop=True)\n",
    "            train_data += data[data_cols][0:training_size].values.tolist()\n",
    "            train_labels += data[label_col][0:training_size].values.tolist()\n",
    "            test_data += data[data_cols][training_size:training_size+test_size].values.tolist()\n",
    "            test_labels += data[label_col][training_size:training_size+test_size].values.tolist()\n",
    "    \n",
    "    # da modellen kun kan trænes med numpy arrays, så skal listerne lige konverteres\n",
    "    train_data = np.asarray(train_data)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    test_data = np.asarray(test_data)\n",
    "    test_labels = np.asarray(test_labels)\n",
    "    \n",
    "    return (train_data, train_labels), (test_data, test_labels)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# kør dette step hvis processed-data ikke er tilgængeligt \n",
    "# og det først skal udvindes fra raw\n",
    "##\n",
    "\n",
    "df_cp = df.copy()\n",
    "# tilføj kategoriske numeriske værdier for genre\n",
    "df_cp.genre = pd.Categorical(df_cp.genre)\n",
    "df_cp['genre_code'] = df_cp.genre.cat.codes\n",
    "# optæl ord i sangtekst\n",
    "df_cp = word_count(df_cp, 'num_words', 'lyrics')\n",
    "# normaliser antal ord i sangtekst\n",
    "df_cp = normalize(df_cp, 'num_words_nm', 'num_words')\n",
    "df_cp = analyze_sentiment(df_cp)\n",
    "# fjern col 'index'\n",
    "df_cp.drop(['index'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# kør dette step hvis processed-data allerede er tilgængeligt\n",
    "##\n",
    "\n",
    "df_cp = pd.read_pickle('./data/clean_with_lan_and_words_and_sent.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FJERNELSE AF UNØDVENDIG DATA ###\n",
    "# fjern nan\n",
    "df_cp.dropna()\n",
    "# fjern uønskede genre (Not Available & Other)\n",
    "df_cp = df_cp[(df_cp.genre != 'Not Available') & (df_cp.genre != 'Other')]\n",
    "# fjern outliers ud fra antal ord i sangtekst\n",
    "df_cp = remove_outliers(df_cp, 'num_words')\n",
    "# reset index\n",
    "df_cp = df_cp.reset_index(drop=True)\n",
    "\n",
    "\n",
    "### --> evt. gem dataset her ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Country\n",
      "6 Metal\n",
      "9 Pop\n",
      "11 Rock\n"
     ]
    }
   ],
   "source": [
    "# klargør data til model\n",
    "(train_data, train_labels), (test_data, test_labels) = prepare_data(df_cp, ['num_words_nm', 'subjectivity', 'polarity'], 'genre_code', 10000, 400)\n",
    "\n",
    "# Vis genre ud fra kategori kode\n",
    "for code in np.unique(test_labels):\n",
    "    print(code, df_cp[df_cp.genre_code == code].genre[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup netværk lag\n",
    "\n",
    "- `input_nodes` er antallet af inputs parametrer/features\n",
    "- `hidden_nodes` anbefalet antal er svarende til et tal mellem input og output nodes\n",
    "- `output_nodes` er antallet af \"labels\" kategorier man forsøger at klassificerer for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 3\n",
    "hidden_nodes = 4\n",
    "output_nodes = 12\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(input_nodes),\n",
    "    keras.layers.Dense(hidden_nodes, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(output_nodes, activation=tf.nn.sigmoid)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile modellen\n",
    "Før modellen er klar til træning, mangler den nogle flere indstillinger. Disse er tilføjet under compiling:\n",
    "\n",
    "- Loss function — Denne måler hvor præcis modellen er under træning. Vi vil minimerer denne funktion til, at \"styre\" modellen i den rigtige retning.\n",
    "- Optimizer — Denne afgører hvordan modellen er opdateret, baseret på det data den ser \n",
    "- Metrics — Brugt til at monitorerer under træningen og testing trin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Træning af modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "40000/40000 [==============================] - 2s 48us/step - loss: 1.7208 - acc: 0.2371: 0s - loss: 1.7652 - acc: 0\n",
      "Epoch 2/40\n",
      "40000/40000 [==============================] - 2s 46us/step - loss: 1.4063 - acc: 0.2497\n",
      "Epoch 3/40\n",
      "40000/40000 [==============================] - 2s 54us/step - loss: 1.3928 - acc: 0.2502\n",
      "Epoch 4/40\n",
      "40000/40000 [==============================] - 2s 46us/step - loss: 1.3892 - acc: 0.2491\n",
      "Epoch 5/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3877 - acc: 0.2464\n",
      "Epoch 6/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3870 - acc: 0.2474\n",
      "Epoch 7/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3867 - acc: 0.2461\n",
      "Epoch 8/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3864 - acc: 0.2559\n",
      "Epoch 9/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3845 - acc: 0.2780\n",
      "Epoch 10/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3739 - acc: 0.2890\n",
      "Epoch 11/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3571 - acc: 0.3243\n",
      "Epoch 12/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3510 - acc: 0.3246\n",
      "Epoch 13/40\n",
      "40000/40000 [==============================] - 2s 43us/step - loss: 1.3491 - acc: 0.3259\n",
      "Epoch 14/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3481 - acc: 0.3235\n",
      "Epoch 15/40\n",
      "40000/40000 [==============================] - 2s 43us/step - loss: 1.3472 - acc: 0.3266\n",
      "Epoch 16/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3466 - acc: 0.3231\n",
      "Epoch 17/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3461 - acc: 0.3226\n",
      "Epoch 18/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3456 - acc: 0.3222\n",
      "Epoch 19/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3452 - acc: 0.3213\n",
      "Epoch 20/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3448 - acc: 0.3221\n",
      "Epoch 21/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3443 - acc: 0.3238\n",
      "Epoch 22/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3441 - acc: 0.3243\n",
      "Epoch 23/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3438 - acc: 0.3271\n",
      "Epoch 24/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3432 - acc: 0.3301: 0s - loss: 1.3432 - acc: 0.330\n",
      "Epoch 25/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3429 - acc: 0.3331\n",
      "Epoch 26/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3425 - acc: 0.3363\n",
      "Epoch 27/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3420 - acc: 0.3421\n",
      "Epoch 28/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3416 - acc: 0.3422\n",
      "Epoch 29/40\n",
      "40000/40000 [==============================] - 2s 40us/step - loss: 1.3410 - acc: 0.3436\n",
      "Epoch 30/40\n",
      "40000/40000 [==============================] - 2s 38us/step - loss: 1.3405 - acc: 0.3463\n",
      "Epoch 31/40\n",
      "40000/40000 [==============================] - 2s 40us/step - loss: 1.3399 - acc: 0.3439\n",
      "Epoch 32/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3395 - acc: 0.3456: 0s - loss: 1.340\n",
      "Epoch 33/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3386 - acc: 0.3469: 0s - loss: \n",
      "Epoch 34/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3381 - acc: 0.3488\n",
      "Epoch 35/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3373 - acc: 0.3476\n",
      "Epoch 36/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3367 - acc: 0.3497\n",
      "Epoch 37/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3357 - acc: 0.3526\n",
      "Epoch 38/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3350 - acc: 0.3510\n",
      "Epoch 39/40\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 1.3342 - acc: 0.3522\n",
      "Epoch 40/40\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 1.3333 - acc: 0.3551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a25a03ac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluerer præcisionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 0s 81us/step\n",
      "Test accuracy: 0.361875\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [1,1], In[1]: [3,3]\n\t [[{{node MatMul_27}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_1_9_0_0, MatMul_27/ReadVariableOp)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-40074713f1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'klassificeringer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1878\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [1,1], In[1]: [3,3]\n\t [[{{node MatMul_27}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_1_9_0_0, MatMul_27/ReadVariableOp)]]"
     ]
    }
   ],
   "source": [
    "input = np.asarray([0.0])\n",
    "prediction = model.predict(input)\n",
    "\n",
    "print('klassificeringer')\n",
    "print(prediction)\n",
    "\n",
    "print(f'model gætter på: {np.argmax(prediction)} som er {df_cp[df_cp.genre_code == np.argmax(prediction)].genre[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
