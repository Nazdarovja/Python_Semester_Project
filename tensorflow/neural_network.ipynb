{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ML\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import zipfile\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasættet i sin \"raw\" form indeholder:\n",
      "362237 antal rækker\n"
     ]
    }
   ],
   "source": [
    "# import raw dataset\n",
    "csv_file = './data/lyrics.csv'\n",
    "\n",
    "if not os.path.isfile(csv_file):\n",
    "    with zipfile.ZipFile(f'{csv_file}.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./data')\n",
    "        \n",
    "df = pd.read_csv(csv_file)\n",
    "rows, _= df.shape\n",
    "print('datasættet i sin \"raw\" form indeholder:')\n",
    "print(f'{rows} antal rækker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktioner til senere brug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, new_col_name, col_to_norm):\n",
    "    '''\n",
    "    ref: https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    max = df[col_to_norm].max()\n",
    "    min = df[col_to_norm].min()\n",
    "\n",
    "    df[new_col_name] = df[col_to_norm].apply(lambda val: (val-min)/(max-min))\n",
    "    return df\n",
    "\n",
    "def _count_words(words):\n",
    "    try:\n",
    "        return len(words.split())\n",
    "    except:\n",
    "        return 0 #TODO: better error handling, maybe not return 0\n",
    "\n",
    "def word_count(df, new_col_name, col_with_lyrics):\n",
    "    df = df.copy()\n",
    "    df[new_col_name] = df[col_with_lyrics].apply(lambda words: _count_words(words))\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df, col_to_process, low=.05, high=.95):\n",
    "    df = df.copy()\n",
    "    min, max = df[col_to_process].quantile([low,high])\n",
    "    df = df[(df[col_to_process] >= min) & (df[col_to_process] <= max)]\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def analyze_sentiment(df):\n",
    "    df = df.copy()\n",
    "    res = df['lyrics'].apply(lambda txt : TextBlob(txt).sentiment)\n",
    "    df['polarity'] = res.apply(lambda x: x[0])\n",
    "    df['subjectivity'] = res.apply(lambda x: x[1])\n",
    "    return df\n",
    "\n",
    "def sentence_avg_word_length(df, new_col_name, col_with_lyrics):\n",
    "    df[new_col_name] = df[col_with_lyrics].astype(str).apply(_sentence_avg_word_length)\n",
    "    return df\n",
    "\n",
    "def _sentence_avg_word_length(sentence):\n",
    "    res = sum(len(word.split()) for word in sentence) / len(sentence.split())**3\n",
    "    return res\n",
    "\n",
    "def analyze_word_class(df):\n",
    "    blobs = df['lyrics'].apply(lambda txt : TextBlob(txt).tags)\n",
    "    df['nouns'] = blobs.apply(lambda word_list: _count_word_class(word_list, 'NN'))\n",
    "    df['adverbs'] = blobs.apply(lambda word_list: _count_word_class(word_list, 'RB'))\n",
    "    df['verbs'] = blobs.apply(lambda word_list: _count_word_class(word_list, 'VB'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _count_word_class(words, word_class):\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w[1] == word_class:\n",
    "            count = count + 1\n",
    "    return count / 100\n",
    "\n",
    "def prepare_data(df, data_cols, label_col, training_size=1000, test_size=250):\n",
    "    labels = df_cp[label_col].value_counts().keys().tolist()\n",
    "    train_data, train_labels, test_data, test_labels = [], [], [], []\n",
    "    \n",
    "    # shuffle dataset\n",
    "    df = df.copy().sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    for label in labels:\n",
    "        data = df[df[label_col] == label]\n",
    "        # kun hvis der er nok eksempler, ift. training_size og test_size, ud fra den pågældende label\n",
    "        if len(data) > training_size + test_size:\n",
    "            data = data.reset_index(drop=True)\n",
    "            train_data += data[data_cols][0:training_size].values.tolist()\n",
    "            train_labels += data[label_col][0:training_size].values.tolist()\n",
    "            test_data += data[data_cols][training_size:training_size+test_size].values.tolist()\n",
    "            test_labels += data[label_col][training_size:training_size+test_size].values.tolist()\n",
    "    \n",
    "    # da modellen kun kan trænes med numpy arrays, så skal listerne lige konverteres\n",
    "    train_data = np.asarray(train_data)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    test_data = np.asarray(test_data)\n",
    "    test_labels = np.asarray(test_labels)\n",
    "    \n",
    "    return (train_data, train_labels), (test_data, test_labels)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### udvind features og tilføj til datasæt\n",
    "Hvis det er første gang dette step køres vær opmærksom på, at der laves tunge sproglige analyser af sangteksterne. Det vil derfor resulterer i, siden der er MANGE rækker data, at det kan tage 40+ minutter at udvinde alle features hvis hele datasættet benyttes. Det kan derfor anbefales, at man tage en mindre sample af datasættet. \n",
    "\n",
    "Hvis dette step ER kørt, så burde der være gemt en `.pkl` i `./data` som gør, at feature genereringen kan springes over. \n",
    "\n",
    "Hvis man gerne vil generere et nyt feature, måske fordi man gerne vil have et mindre datasæt ved ændring af `sample` variablen, så lav om `FEATURE_DATASET_FILE` variablen, så det gamle feature datasæt kan beholdes. Der skal ligges mærke til, at når der udtages sample af datasættet, så ligges de enkelte udvalgte rækker tilbage. Dette er en IKKE en rigtig måde at gøre det på, da der så kan fremkomme duplikationer af rækker. Det er derimod for, at lave et \"proof of concept\" med mulighed for, at benytte alle genre i træning af modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading feature dataset from pickle...DONE\n"
     ]
    }
   ],
   "source": [
    "FEATURE_DATASET_FILE = './data/feature_dataset.pkl'\n",
    "SAMPLE_SIZE = 1000 # <-- ændre denne hvis et anden størrelse datasæt ønskes\n",
    "\n",
    "if not os.path.isfile(FEATURE_DATASET_FILE):\n",
    "    df_cp = df.copy() # <-- kopi af importeret datasæt\n",
    "\n",
    "    ###\n",
    "    # Oprydning i raw datasæt\n",
    "    ###\n",
    "    # fjern rækker med nan værdier\n",
    "    df_cp = df_cp.dropna()\n",
    "\n",
    "    # fjern uønskede genre (Not Available & Other)\n",
    "    df_cp = df_cp[(df_cp.genre != 'Not Available') & (df_cp.genre != 'Other')]\n",
    "\n",
    "    ###\n",
    "    # Udtaget stikprøve af datasæt\n",
    "    ###\n",
    "    grouped = df_cp.groupby('genre')\n",
    "    grouped = grouped.apply(lambda x: x.sample(n=SAMPLE_SIZE, replace=True))\n",
    "    df_cp = grouped.reset_index(drop=True)\n",
    "    \n",
    "    ###\n",
    "    # Tilføj features til datasæt\n",
    "    ###\n",
    "    # tilføj kategoriske numeriske værdier for genre\n",
    "    print('generate categorical values for genre...', end='')\n",
    "    df_cp.genre = pd.Categorical(df_cp.genre)\n",
    "    df_cp['genre_code'] = df_cp.genre.cat.codes\n",
    "    print('DONE')\n",
    "    \n",
    "    # optæl ord i sangtekst\n",
    "    print('generate number-of-words feature and normalize...', end='')\n",
    "    df_cp = word_count(df_cp, 'num_words', 'lyrics')\n",
    "    # normaliser antal ord i sangtekst\n",
    "    df_cp = normalize(df_cp, 'num_words_nm', 'num_words')\n",
    "    # fjern outliers ud fra antal ord i sangtekst\n",
    "    df_cp = remove_outliers(df_cp, 'num_words')\n",
    "    print('DONE')\n",
    "    \n",
    "    # optæl gennemsnitlig ordlængde i sangtekst\n",
    "    print('generate avg-word-length feature and normalize...', end='')\n",
    "    #df_cp = df_cp[df_cp.lyrics.apply(type) == str] # lyrics MUST be type string\n",
    "    df_cp = sentence_avg_word_length(df_cp, 'avg_word_len', 'lyrics')\n",
    "    df_cp = normalize(df_cp, 'avg_word_len_nm', 'avg_word_len')\n",
    "    print('DONE')\n",
    "    \n",
    "    # sentiments analyse\n",
    "    print('generate sentiments analyzis...', end='')\n",
    "    df_cp = analyze_sentiment(df_cp)\n",
    "    print('DONE')\n",
    "    \n",
    "    # optæl ord-klasser i sangtekst\n",
    "    print('generate word-class counts...', end='')\n",
    "    df_cp = analyze_word_class(df_cp)\n",
    "    print('DONE')\n",
    "    \n",
    "    # fjern col 'index'\n",
    "    df_cp.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    print('saving feature dataset to pickle...', end='')\n",
    "    df_cp.to_pickle(FEATURE_DATASET_FILE)\n",
    "    print('DONE')\n",
    "else:\n",
    "    print('reading feature dataset from pickle...', end='')\n",
    "    df_cp = pd.read_pickle(FEATURE_DATASET_FILE)\n",
    "    print('DONE')\n",
    "\n",
    "max_nw, min_nw = df_cp.num_words.max(), df_cp.num_words.min()\n",
    "max_awl, min_awl = df_cp.avg_word_len.max(), df_cp.avg_word_len.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klargør træning og test data/labels\n",
    "Da modellen ikke skal testes på data den også trænes på, for ikke at give et misvisende billede af hvor god modellen er til, at genkende genre fra en sangtekst den ikke har set før.\n",
    "\n",
    "Der er flere variabler der gerne må ændres på for, at se hvilket resultat det giver til modellen evne til, at genkende genren:\n",
    "\n",
    "- `features` er en liste med de karakteristika som modellen skal lærer fra. Det er ikke altid sikker, at alle features bidrager til en bedre evne til, at genkende genren. Derfor kan der fjernes fra denne `list` variabel.\n",
    "- `train_size`/`test_size` bestemmer hvor stort et datasæt modellen skal trænes/testes på. Da ikke alle genre optræder i datasættet lige mange gange vil, hvis man vælger en for stor `train_size` + `test_size`, bestemte genre ikke blive tilføjet til train/test datasættet da `(antal af sangtekste fra en bestemt genre) >= train_size + test_size`. Dette sørger `prepare_data()` for at blive realiseret.\n",
    "\n",
    "Der vil, efter nedestående blok bliver kørt, blive vist hvilke genre modellen trænes i, at kunne genkende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre modellen trænes til at genkende samt hvilken genre_code genren har:\n",
      "0 Country\n",
      "1 Electronic\n",
      "2 Folk\n",
      "3 Hip-Hop\n",
      "4 Indie\n",
      "5 Jazz\n",
      "6 Metal\n",
      "7 Pop\n",
      "8 R&B\n",
      "9 Rock\n"
     ]
    }
   ],
   "source": [
    "features = ['num_words_nm', 'avg_word_len_nm', 'subjectivity', 'polarity', 'nouns', 'adverbs', 'verbs']\n",
    "output_labels = 'genre_code'\n",
    "train_size = 100\n",
    "test_size = 20\n",
    "\n",
    "# klargør data til model\n",
    "(train_data, train_labels), (test_data, test_labels) = prepare_data(df_cp, features, output_labels, train_size, test_size)\n",
    "\n",
    "# Vis genre ud fra kategori kode\n",
    "print('Genre modellen trænes til at genkende samt hvilken genre_code genren har:')\n",
    "for code in np.unique(test_labels):\n",
    "    print(code, df_cp[df_cp.genre_code == code].genre[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup netværk lag\n",
    "\n",
    "- `input_nodes` er antallet af inputs parametrer/features\n",
    "- `hidden_nodes` anbefalet antal er svarende til et tal mellem input og output nodes\n",
    "- `output_nodes` er antallet af \"labels\" kategorier man forsøger at klassificerer for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = len(features)\n",
    "hidden_nodes = 4\n",
    "output_nodes = 12\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(input_nodes),\n",
    "    keras.layers.Dense(hidden_nodes, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(output_nodes, activation=tf.nn.sigmoid)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile modellen\n",
    "Før modellen er klar til træning, mangler den nogle flere indstillinger. Disse er tilføjet under compiling:\n",
    "\n",
    "- Loss function — Denne måler hvor præcis modellen er under træning. Vi vil minimerer denne funktion til, at \"styre\" modellen i den rigtige retning.\n",
    "- Optimizer — Denne afgører hvordan modellen er opdateret, baseret på det data den ser \n",
    "- Metrics — Brugt til at monitorerer under træningen og testing trin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Træning af modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 2.4652 - acc: 0.1000\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 2.4521 - acc: 0.1000\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.4396 - acc: 0.1000\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.4276 - acc: 0.1000\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.4160 - acc: 0.1000\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.4050 - acc: 0.1000\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3944 - acc: 0.1000\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.3846 - acc: 0.1000\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3751 - acc: 0.1000\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3663 - acc: 0.1000\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 2.3586 - acc: 0.1000\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 2.3516 - acc: 0.1000\n",
      "Epoch 13/40\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 2.3456 - acc: 0.1000\n",
      "Epoch 14/40\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.3402 - acc: 0.1000\n",
      "Epoch 15/40\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.3356 - acc: 0.1000\n",
      "Epoch 16/40\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.3319 - acc: 0.1000\n",
      "Epoch 17/40\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.3284 - acc: 0.1000\n",
      "Epoch 18/40\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 2.3255 - acc: 0.1000\n",
      "Epoch 19/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3232 - acc: 0.1000\n",
      "Epoch 20/40\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.3211 - acc: 0.1000\n",
      "Epoch 21/40\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.3193 - acc: 0.1000\n",
      "Epoch 22/40\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.3179 - acc: 0.1000\n",
      "Epoch 23/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3165 - acc: 0.1000\n",
      "Epoch 24/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3154 - acc: 0.0990\n",
      "Epoch 25/40\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.3144 - acc: 0.0980\n",
      "Epoch 26/40\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 2.3135 - acc: 0.0990\n",
      "Epoch 27/40\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 2.3127 - acc: 0.0980\n",
      "Epoch 28/40\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.3122 - acc: 0.0860\n",
      "Epoch 29/40\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.3115 - acc: 0.0880\n",
      "Epoch 30/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3109 - acc: 0.0860\n",
      "Epoch 31/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3103 - acc: 0.0920\n",
      "Epoch 32/40\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.3093 - acc: 0.093 - 0s 26us/step - loss: 2.3100 - acc: 0.0860\n",
      "Epoch 33/40\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3096 - acc: 0.0840\n",
      "Epoch 34/40\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 2.3093 - acc: 0.0670\n",
      "Epoch 35/40\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.3089 - acc: 0.0660\n",
      "Epoch 36/40\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 2.3086 - acc: 0.0680\n",
      "Epoch 37/40\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.3084 - acc: 0.0980\n",
      "Epoch 38/40\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.3081 - acc: 0.0990\n",
      "Epoch 39/40\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 2.3079 - acc: 0.0760\n",
      "Epoch 40/40\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.3075 - acc: 0.1020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bf1e45f630>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluerer præcisionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 244us/step\n",
      "Test accuracy: 0.105\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate number-of-words feature and normalize...DONE\n",
      "generate avg-word-length feature and normalize...DONE\n",
      "generate sentiments analyzis...DONE\n",
      "generate word-class counts...DONE\n",
      "\n",
      "model gætter på: 5 som er Jazz\n"
     ]
    }
   ],
   "source": [
    "lyric = '''\n",
    "input din sangtekst her!!\n",
    "'''\n",
    "\n",
    "\n",
    "dic = {'lyrics': [lyric]}\n",
    "predict_df = pd.DataFrame(dic)\n",
    "    \n",
    "# optæl ord i sangtekst\n",
    "print('generate number-of-words feature and normalize...', end='')\n",
    "predict_df = word_count(predict_df, 'num_words', 'lyrics')\n",
    "# normaliser antal ord i sangtekst\n",
    "predict_df['num_words_nm'] = predict_df.num_words.apply(lambda val: (val-min_nw)/(max_nw-min_nw))\n",
    "print('DONE')\n",
    "    \n",
    "# optæl gennemsnitlig ordlængde i sangtekst\n",
    "print('generate avg-word-length feature and normalize...', end='')\n",
    "#df_cp = df_cp[df_cp.lyrics.apply(type) == str] # lyrics MUST be type string\n",
    "predict_df = sentence_avg_word_length(predict_df, 'avg_word_len', 'lyrics')\n",
    "predict_df['avg_word_len_nm'] = predict_df.avg_word_len.apply(lambda val: (val-min_awl)/(max_awl-min_awl))\n",
    "print('DONE')\n",
    "    \n",
    "# sentiments analyse\n",
    "print('generate sentiments analyzis...', end='')\n",
    "predict_df = analyze_sentiment(predict_df)\n",
    "print('DONE')\n",
    "    \n",
    "# optæl ord-klasser i sangtekst\n",
    "print('generate word-class counts...', end='')\n",
    "predict_df = analyze_word_class(predict_df)\n",
    "print('DONE')\n",
    "\n",
    "input = predict_df[['num_words_nm','avg_word_len_nm','polarity', 'subjectivity', 'nouns', 'adverbs', 'verbs']]\n",
    "input = np.asarray(input)\n",
    "prediction = model.predict(input)\n",
    "\n",
    "print()\n",
    "print(f'model gætter på: {np.argmax(prediction)} som er {df_cp[df_cp.genre_code == np.argmax(prediction)].genre[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
